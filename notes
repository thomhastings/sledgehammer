Notes

Initial Version Notes
---------------------

Do you provide a username to the tool and it tries different passwords,  or does it try usernames and passwords from the dictionary files?
    -Both, different modes.



itertools.product VS. Nested for loop profiling

Both tested with 500 worst passwords file as usernames and passwords.
File from here:
http://www.skullsecurity.org/wiki/index.php/Passwords

Both times try_credentials was just stub that prints to console   

Using itertools.product
        783721 function calls (782683 primitive calls) in 16.953 seconds

        810.643 when tested against VM with full implementation of try_credentials

Using nested for/in
        534721 function calls (533683 primitive calls) in 16.933 seconds

        838.779 when tested against VM with full implementation of try_credentials


Problem with max retries / socket error:

requests.exceptions.ConnectionError: HTTPConnectionPool(host='owaspbwa', port=80): Max retries exceeded with url: /WebGoat/attack (Caused by <class 'socket.error'>: [Errno 99] Cannot assign requested address)

This error would occur during runs with long word lists.  Short word lists that completed quickly would not experience it.

The exception messages are not actually very clear on this and it left me confused as to the cause. It occured to me that the server might just be rejecting my connections.  But throttling my requests didn't solve the issue.

After further searching I found this in the requests module's documentation:


"Excellent news â€” thanks to urllib3, keep-alive is 100% automatic within a session! Any requests that you make within a session will automatically reuse the appropriate connection!

Note that connections are only released back to the pool for reuse once all body data has been read; be sure to either set stream to False or read the content property of the Response object."

I believe that since I'm not actually reading the content of the Request object the connections are not actually bein returned to the connection pool and I'm exhausting the number of connections in the connection pool.

Adding an HTTP header of 'Connection: close' to the request prevented the error from occuring.


Adding Threading for HTTP requests
==================================

Do I need to join my threads? 

Am I doing any processing in the *main* thread that requires me to wait for them all to finish so I can do operations / evaluations on there results?

    Don't think so.  The function being run in the threads does all the evaluation internally (good creds or not) and I don't think anything needs to be done after they finish.  Just fire them off and the program will end once all the dictionary words have been tried.

    See http://stackoverflow.com/questions/3088449/explicit-joining-of-python-threads?rq=1

Should I limit the number of threads? Like some kind of thread pool?

    I think this isn't necessary.  As each HTTP request completes and the threaded function ends, that thread dies.  So as I create new ones, the older finished ones have ended.  Maybe there's a start up cost of creating threads that can be avoided by using a pool.  Will have to look into this further.

Use multiple threads to read from the word list as well as to make the HTTP requests?  Am I not I/O bound on the file reading itself as well as the network I/O of the HTTP requests?

    Have found conflicting info on whether having multiple threads reading from the file is actually a benefit.  Some stackoverflow posts suggest that it might actually be detrimental:

    1st comment on original question here http://stackoverflow.com/questions/9175914/reading-a-single-file-from-multiple-threads-in-python

    http://stackoverflow.com/questions/17972007/in-c-how-to-read-one-file-with-multiple-threads

    Honestly, I think this is adding unnecessary complexity to this iteration of the program.  If anything I should consider pre-caching the whole word list into memory rather than doing a readline() each go around.

    Will probably go with main thread feeding data into a Queue and consumer threads doing the requests and evaluating the responses.





































